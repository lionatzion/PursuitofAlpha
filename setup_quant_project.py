# -*- coding: utf-8 -*-
"""setup_quant_project.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z4_gcDqzwrUJOjyy6RQLOtp_B-nF-rEm
"""

# setup_quant_project.py
# Run this in Google Colab (or any Python environment with Drive mounted)
import os
import shutil

# 1) Mount Google Drive (only in Colab)
try:
    from google.colab import drive
    drive.mount('/content/drive')
    ROOT = "/content/drive/MyDrive/quant_backtest_ml"
except ImportError:
    # If not in Colab, adjust the path to your local Drive mount
    ROOT = os.path.expanduser("~/Google Drive/quant_backtest_ml")

print("Project root:", ROOT)

# 2) Create folder structure
folders = [
    "data/raw",
    "data/processed",
    "data/altdata",
    "models/trained",
    "models/hf/finbert",
    "models/hf/finbert-tone",
    "modules",
    "pipelines",
    "notebooks",
    "logs"
]
for sub in folders:
    path = os.path.join(ROOT, sub)
    os.makedirs(path, exist_ok=True)
    print("✔️", path)

# 3) Write top-level placeholder files
# 3.1 README.md
with open(os.path.join(ROOT, "README.md"), "w") as f:
    f.write(
        "# Quant Backtest ML\n\n"
        "This Medallion-style quant project lives in this folder.\n\n"
        "## Structure\n\n"
        "- `data/`: raw, processed, altdata\n"
        "- `models/`: trained, hf (FinBERT)\n"
        "- `modules/`: Python modules\n"
        "- `pipelines/`: end-to-end scripts\n"
        "- `notebooks/`: Colab & Jupyter notebooks\n"
        "- `logs/`: run logs\n"
    )

# 3.2 requirements.txt
with open(os.path.join(ROOT, "requirements.txt"), "w") as f:
    f.write("\n".join([
        "pandas",
        "numpy",
        "yfinance",
        "scikit-learn",
        "backtrader",
        "ta",
        "transformers",
        "torch",
        "joblib",
    ]))

# 3.3 .env_template
with open(os.path.join(ROOT, ".env_template"), "w") as f:
    f.write(
        "# Copy to .env and fill your keys\n"
        "HUGGINGFACE_TOKEN=\n"
        "TIINGO_API_KEY=\n"
        "POLYGON_API_KEY=\n"
        "NEWSAPI_KEY=\n"
    )

# 3.4 config.yaml
with open(os.path.join(ROOT, "config.yaml"), "w") as f:
    f.write(
        "project_root: \"{ROOT}\"\n"
        "data:\n"
        "  raw: \"data/raw\"\n"
        "  processed: \"data/processed\"\n"
        "  alt: \"data/altdata\"\n"
        "models:\n"
        "  hf:\n"
        "    finbert: \"models/hf/finbert\"\n"
        "    tone: \"models/hf/finbert-tone\"\n"
        "  trained: \"models/trained\"\n"
        "tickers:\n"
        "  - \"AAPL\"\n"
        "  - \"SPY\"\n"
        "backtest:\n"
        "  start_date: \"2019-01-01\"\n"
        "  end_date:   \"2024-12-31\"\n"
        "  interval:   \"1h\"\n"
    )

# 3.5 Dockerfile
with open(os.path.join(ROOT, "Dockerfile"), "w") as f:
    f.write(
        "FROM python:3.10-slim\n\n"
        "WORKDIR /app\n"
        "COPY requirements.txt .\n"
        "RUN pip install --no-cache-dir -r requirements.txt\n\n"
        "COPY . .\n"
        "CMD [\"python\", \"main.py\"]\n"
    )

# 4) Copy in your existing backtesting_workflow.py
src = "/content/backtesting_workflow.py"  # or wherever you first upload it
dst = os.path.join(ROOT, "modules", "backtesting_workflow.py")
if os.path.isfile(src):
    shutil.copy(src, dst)
    print("Copied backtesting_workflow.py → modules/")
else:
    print("⚠️ Please upload backtesting_workflow.py to /content before running.")

# 5) Write minimal boilerplate for other modules
module_boilerplates = {
    "data_ingestion.py":
    '''"""
data_ingestion.py

Functions to download data from yfinance, Quandl, etc.
"""
import yfinance as yf
import pandas as pd

def download_data(tickers, start, end, interval="1h"):
    data = {}
    for t in tickers:
        df = yf.download(t, start=start, end=end, interval=interval, progress=False)
        if not df.empty:
            df.dropna(inplace=True)
            data[t] = df
    return data
''',

    "feature_engineering.py":
    '''"""
feature_engineering.py

Create volume bars and add technical indicators.
"""
import pandas as pd
import numpy as np
from ta import volatility

def create_volume_bars(df, vol_target=1e6):
    # ... your logic ...
    return pd.DataFrame()

def add_indicators(df, sma_fast=10, sma_slow=30, rsi_window=14, z_window=50):
    # ... your logic ...
    return df
''',

    "model_training.py":
    '''"""
model_training.py

Prepare features, train classifiers, and save models.
"""
import joblib
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

def prepare_features(df, feature_cols, lookahead=3):
    # ... your logic ...
    return None, None

def train_classifier(X, y, **kwargs):
    # ... your logic ...
    return None, 0.0

def save_model(model, path):
    joblib.dump(model, path)
''',

    "sentiment_analysis.py":
    '''"""
sentiment_analysis.py

Load FinBERT pipelines and score text.
"""
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline

def load_finbert(model_dir):
    model = AutoModelForSequenceClassification.from_pretrained(model_dir)
    tok   = AutoTokenizer.from_pretrained(model_dir)
    return pipeline("sentiment-analysis", model=model, tokenizer=tok)

def score_sentiment(texts, sent_pipe):
    return [sent_pipe(t)[0]['label'] for t in texts]
'''
}

for fname, txt in module_boilerplates.items():
    path = os.path.join(ROOT, "modules", fname)
    with open(path, "w") as f:
        f.write(txt)
    print("✔️ Boilerplate:", fname)

# 6) Write pipeline scripts
pipe_dir = os.path.join(ROOT, "pipelines")
pipelines = {
    "train_pipeline.py":
    '''"""
train_pipeline.py

End-to-end: load data → features → train model → save.
"""
from modules.data_ingestion import download_data
from modules.feature_engineering import add_indicators, create_volume_bars
from modules.model_training import prepare_features, train_classifier, save_model
import os

def main():
    tickers = ["AAPL"]
    data = download_data(tickers, "2019-01-01", "2024-12-31")
    # ... feature & train ...
    save_model(None, os.path.join("..","models","trained","model.joblib"))

if __name__=="__main__":
    main()
'''
,
    "backtest_pipeline.py":
    '''"""
backtest_pipeline.py

Load model & data → run backtest → output.
"""
import joblib
from modules.backtesting_workflow import run_backtest

def main():
    model = joblib.load("../models/trained/model.joblib")
    # load df ...
    result = run_backtest(model, None)
    print("Final value:", result)

if __name__=="__main__":
    main()
'''
}

for fname, txt in pipelines.items():
    path = os.path.join(pipe_dir, fname)
    with open(path, "w") as f:
        f.write(txt)
    print("✔️ Pipeline:", fname)

# 7) Create blank notebook stub
nb_stub = "{}"
with open(os.path.join(ROOT, "notebooks", "quant_backtest_setup.ipynb"), "w") as f:
    f.write(nb_stub)
print("✔️ Created notebook stub")

!python setup_quant_project.py

from google.colab import drive
drive.mount('/content/drive')

import os

# Point to your project folder on Drive
PROJECT_ROOT = "/content/drive/MyDrive/quant_backtest_ml"

# List of subfolders to create
folders = [
    "data/raw",
    "data/processed",
    "data/altdata",
    "models/trained",
    "models/hf/finbert",
    "models/hf/finbert-tone",
    "modules",
    "pipelines",
    "notebooks",
    "logs"
]

# Make them
for sub in folders:
    path = os.path.join(PROJECT_ROOT, sub)
    os.makedirs(path, exist_ok=True)
    print("✔️", path)

import shutil

src = "/content/backtesting_workflow.py"
dst = os.path.join(PROJECT_ROOT, "modules", "backtesting_workflow.py")

if os.path.exists(src):
    shutil.copy(src, dst)
    print("✔️ Copied backtesting_workflow.py → modules/")
else:
    print("⚠️ Please upload backtesting_workflow.py first.")

# README.md
with open(os.path.join(PROJECT_ROOT, "README.md"), "w") as f:
    f.write(
        "# Quant Backtest ML\n\n"
        "Medallion-style quant project.\n\n"
        "## Structure\n"
        "- data/: raw, processed, altdata\n"
        "- models/: trained, hf (FinBERT)\n"
        "- modules/: Python code\n"
        "- pipelines/: scripts\n"
        "- notebooks/: Colab & Jupyter\n"
        "- logs/: run outputs\n"
    )

# requirements.txt
with open(os.path.join(PROJECT_ROOT, "requirements.txt"), "w") as f:
    f.write("\n".join([
        "pandas","numpy","yfinance","scikit-learn",
        "backtrader","ta","transformers","torch","joblib"
    ]))

# .env_template
with open(os.path.join(PROJECT_ROOT, ".env_template"), "w") as f:
    f.write(
        "# Copy to .env and fill keys\n"
        "HUGGINGFACE_TOKEN=\n"
        "TIINGO_API_KEY=\n"
        "POLYGON_API_KEY=\n"
        "NEWSAPI_KEY=\n"
    )

# config.yaml
cfg = f"""\
project_root: "{PROJECT_ROOT}"
data:
  raw: "data/raw"
  processed: "data/processed"
  alt: "data/altdata"
models:
  hf:
    finbert: "models/hf/finbert"
    tone: "models/hf/finbert-tone"
  trained: "models/trained"
tickers:
  - "AAPL"
  - "SPY"
backtest:
  start_date: "2019-01-01"
  end_date:   "2024-12-31"
  interval:   "1h"
"""
with open(os.path.join(PROJECT_ROOT, "config.yaml"), "w") as f:
    f.write(cfg)

# Dockerfile
dockerfile = """\
FROM python:3.10-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["python", "main.py"]
"""
with open(os.path.join(PROJECT_ROOT, "Dockerfile"), "w") as f:
    f.write(dockerfile)

print("✔️ Placeholder files written")

boilerplates = {
    "data_ingestion.py": '''"""
data_ingestion.py
Functions to download data from yfinance.
"""
import yfinance as yf

def download_data(tickers, start, end, interval="1h"):
    data={}
    for t in tickers:
        df=yf.download(t, start=start, end=end, interval=interval, progress=False)
        if not df.empty:
            df.dropna(inplace=True)
            data[t]=df
    return data
''',
    "feature_engineering.py": '''"""
feature_engineering.py
Volume bars + indicators.
"""
import pandas as pd, numpy as np
from ta import volatility

def create_volume_bars(df, vol_target=1e6):
    return pd.DataFrame()

def add_indicators(df, sma_fast=10, sma_slow=30, rsi_window=14, z_window=50):
    return df
''',
    "model_training.py": '''"""
model_training.py
Train ML models & save.
"""
import joblib
from sklearn.ensemble import GradientBoostingClassifier

def prepare_features(df, cols, lookahead=3):
    return None, None

def train_classifier(X, y):
    return None, 0.0

def save_model(model, path):
    joblib.dump(model, path)
''',
    "sentiment_analysis.py": '''"""
sentiment_analysis.py
Load FinBERT & score text.
"""
from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline

def load_finbert(model_dir):
    model=AutoModelForSequenceClassification.from_pretrained(model_dir)
    tok=AutoTokenizer.from_pretrained(model_dir)
    return pipeline("sentiment-analysis", model=model, tokenizer=tok)

def score_sentiment(texts, pipe):
    return [pipe(t)[0]['label'] for t in texts]
'''
}

for fn, code in boilerplates.items():
    path = os.path.join(PROJECT_ROOT, "modules", fn)
    with open(path, "w") as f:
        f.write(code)
    print("✔️", fn, "written")

pipelines = {
  "train_pipeline.py": '''"""
train_pipeline.py
Load → feature → train → save.
"""
from modules.data_ingestion import download_data
from modules.feature_engineering import add_indicators, create_volume_bars
from modules.model_training import train_classifier, save_model

def main():
    tickers=["AAPL"]
    data=download_data(tickers,"2019-01-01","2024-12-31")
    # ...
    save_model(None,"../models/trained/model.joblib")

if __name__=="__main__":
    main()
''',
  "backtest_pipeline.py": '''"""
backtest_pipeline.py
Run backtest with trained model.
"""
import joblib
from modules.backtesting_workflow import run_backtest

def main():
    model=joblib.load("../models/trained/model.joblib")
    # df=...
    result=run_backtest(model,None)
    print("Final:",result)

if __name__=="__main__":
    main()
'''
}

for fn, code in pipelines.items():
    path = os.path.join(PROJECT_ROOT, "pipelines", fn)
    with open(path, "w") as f:
        f.write(code)
    print("✔️", fn, "written")